{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.3 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from time import sleep\n",
    "from xml.dom import minidom\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import timm\n",
    "from sklearn import preprocessing, metrics\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "import torch.nn.functional as F\n",
    "from catalyst.data.sampler import BalanceClassSampler, MiniEpochSampler\n",
    "from ranger import Ranger\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import albumentations as A\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90, CoarseDropout,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed everything\n",
    "SEED = 55555\n",
    "\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "DATA_PATH = r\"./train/\"\n",
    "\n",
    "IMAGE_SIZE = 300\n",
    "BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION_STEPS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Running on GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    logging.info(f\"Running on {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    logging.info(\"Running on a CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(quality_threshold=2, data_path=DATA_PATH, rewrite=False):\n",
    "    if not rewrite:\n",
    "        try:\n",
    "            dataset = pd.read_csv(\"dataset.csv\")\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        image_qualities = []\n",
    "\n",
    "        for xmldoc in tqdm_notebook(os.listdir(data_path)):\n",
    "            if xmldoc.__contains__(\".xml\"):\n",
    "                with open(data_path + xmldoc, \"r\", encoding=\"UTF-8\") as f:\n",
    "                    doc = f.read()\n",
    "                doc = minidom.parseString(doc)\n",
    "\n",
    "                label = doc.getElementsByTagName('ClassId')\n",
    "                label = int(label[0].firstChild.nodeValue)\n",
    "\n",
    "                image_path = doc.getElementsByTagName('FileName')\n",
    "                image_path = image_path[0].firstChild.nodeValue            \n",
    "\n",
    "                image_quality = doc.getElementsByTagName('Vote')\n",
    "                image_quality = int(image_quality[0].firstChild.nodeValue)\n",
    "                \n",
    "                if image_quality > quality_threshold:\n",
    "                    image_paths.append(data_path + image_path)\n",
    "                    labels.append(label)\n",
    "                    image_qualities.append(image_quality)\n",
    "\n",
    "                dataset = pd.DataFrame({\"image_path\" : image_paths,\n",
    "                                        \"label\" : labels,\n",
    "                                        \"image_quality\" : image_quality})\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = build_dataset(rewrite=False)\n",
    "dataset.to_csv(\"dataset.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "dataset[\"label\"] = le.fit_transform(dataset[\"label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(dataset[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excl_labels = []\n",
    "# for label in dataset[\"label\"].unique():\n",
    "#     if dataset[\"label\"].value_counts()[label] < len(dataset) * 0.001:\n",
    "#         excl_labels.append(label)\n",
    "\n",
    "# for label in excl_labels:\n",
    "#     dataset = dataset[dataset[\"label\"] != label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"label\"].value_counts().plot.bar(figsize=(30,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "N_CLASS = len(dataset[\"label\"].value_counts())\n",
    "N_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(dataset, shuffle=True, test_size=0.1, random_state=SEED, stratify=dataset[\"label\"])\n",
    "train_dataset, valid_dataset = train_test_split(train_dataset, shuffle=True, test_size=0.15, random_state=SEED, stratify=train_dataset[\"label\"])\n",
    "train_dataset_labels = train_dataset[\"label\"].values\n",
    "test_dataset_labels = test_dataset[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_dataset[\"label\"].value_counts()) == len(test_dataset[\"label\"].value_counts())\n",
    "assert len(train_dataset[\"label\"].value_counts()) == len(valid_dataset[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_paths = dataset['image_path'].values\n",
    "        self.labels = dataset['label'].values\n",
    "        self.image_qualities = dataset['image_quality'].values\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        image, label = self.image_paths[index], self.labels[index]        \n",
    "        \n",
    "        image = cv2.imread(image)\n",
    "        # fast BGR to RGB\n",
    "        image = image[: , : , ::-1]\n",
    "        if image.shape[-1] == 4:\n",
    "            # removing alpha channel if present\n",
    "            image = image[..., :3]\n",
    "        if len(image.shape) == 2:\n",
    "            # converting single channel image to 3 channel for possible greyscales\n",
    "            image = np.stack((image,)  * 3, axis = -1)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image)\n",
    "\n",
    "            image = transformed[\"image\"]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "transform = Compose([\n",
    "            Resize(width=IMAGE_SIZE, height=IMAGE_SIZE),\n",
    "            HorizontalFlip(p=0.4),\n",
    "            ShiftScaleRotate(p=0.3),\n",
    "            MedianBlur(blur_limit=7, always_apply=False, p=0.3),\n",
    "            IAAAdditiveGaussianNoise(scale=(0, 0.15*255), p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.4),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            CoarseDropout(p=0.5),\n",
    "            Cutout(p=0.4),\n",
    "            ToTensorV2(p=1.0),\n",
    "            ], p=1.0)\n",
    "\n",
    "test_transform = Compose([\n",
    "            # only resize and normalization is used for testing\n",
    "            # no TTA is implemented in this solution\n",
    "            Resize(width=IMAGE_SIZE, height=IMAGE_SIZE),                    \n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "            ], p=1.0)\n",
    "\n",
    "\n",
    "train_dataset, valid_dataset = Dataset(train_dataset, transforms = transform), Dataset(valid_dataset, transforms = transform)\n",
    "test_dataset = Dataset(test_dataset, transforms = test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ufoym/imbalanced-dataset-sampler\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, dataset, num_samples: int = None):\n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(self.indices) if num_samples is None else num_samples\n",
    "\n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            label_to_count[label] = label_to_count.get(label, 0) + 1\n",
    "        \n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        return dataset.labels[idx]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                            #balanced sampler is used to minimize harmful effects of dataset not being fully balanced \n",
    "                            #sampler=BalanceClassSampler(labels=train_dataset_labels, mode=\"upsampling\"),           \n",
    "                            sampler=ImbalancedDatasetSampler(train_dataset, num_samples=10000),\n",
    "                            batch_size=BATCH_SIZE)\n",
    "valid_dataloader = DataLoader(\n",
    "                            valid_dataset,\n",
    "                            sampler=ImbalancedDatasetSampler(valid_dataset, num_samples=2000),\n",
    "                            batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(\n",
    "                            test_dataset,\n",
    "                            sampler=SequentialSampler(test_dataset),\n",
    "                            batch_size=1)\n",
    "test_dataloader_balanced = DataLoader(\n",
    "                            test_dataset,\n",
    "                            #sampler=ImbalancedDatasetSampler(test_dataset, num_samples=2000),\n",
    "                            sampler=BalanceClassSampler(labels=test_dataset_labels, mode=\"downsampling\"),\n",
    "                            batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        self.model.drop_rate = 0.25\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        #x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "net = Classifier(\"tf_efficientnet_b4_ns\", N_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ranger optimizer loaded. \nGradient Centralization usage = True\nGC applied to both conv and fc layers\n"
     ]
    }
   ],
   "source": [
    "loss_function  = nn.CrossEntropyLoss()\n",
    "optimizer = Ranger(net.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.9599))\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, EPOCHS * 0.25, LEARNING_RATE * 0.0001)\n",
    "scheduler.last_epoch = EPOCHS\n",
    "scaler = GradScaler()\n",
    "\n",
    "def training_loop():\n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    \n",
    "    for epoch in (range(EPOCHS)):        \n",
    "        if epoch != 0 and epoch > 0.25 * EPOCHS: # cosine anneal the last 25% of epochs\n",
    "            scheduler.step()\n",
    "        logging.info(f\"Epoch {epoch+1}\")\n",
    "\n",
    "        logging.info(\"Training\")\n",
    "        train_losses, train_accuracies, train_f1 = forward_pass(train_dataloader, train = True)\n",
    "\n",
    "        logging.info(\"Validating\")\n",
    "        val_losses, val_accuracies, val_f1 = forward_pass(valid_dataloader)\n",
    "\n",
    "        logging.info(f\"Training accuracy: {sum(train_accuracies)/len(train_accuracies):.2f} | Training loss: {sum(train_losses)/len(train_losses):.2f}\")\n",
    "        logging.info(f\"Training f1:   {sum(train_f1)/len(train_f1):.2f}\")\n",
    "        logging.info(f\"Validation accuracy: {sum(val_accuracies)/len(val_accuracies):.2f} | Validation loss: {sum(val_losses)/len(val_losses):.2f}\")\n",
    "        logging.info(f\"Validation f1:   {sum(val_f1)/len(val_f1):.2f}\")\n",
    "        \n",
    "        epoch_val_loss = sum(val_losses)/len(val_losses)\n",
    "        \n",
    "        if best_loss > epoch_val_loss:\n",
    "            best_model_wts = copy.deepcopy(net.state_dict())\n",
    "            torch.save(net.state_dict(), \"best.pth\")\n",
    "            logging.info(f\"Saving with loss of {epoch_val_loss}, improved over previous {best_loss}\")\n",
    "            best_loss = epoch_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(dataloader, train=False):\n",
    "    if train:\n",
    "        net.train()\n",
    "    else:\n",
    "        net.eval()\n",
    "\n",
    "    losses = deque(maxlen=5000)\n",
    "    accuracies = deque(maxlen=5000)\n",
    "    f1_scores = deque(maxlen=5000)\n",
    "\n",
    "    for step, batch in (enumerate(dataloader)):\n",
    "        inputs = batch[0].to(device).float()\n",
    "        labels = batch[1].to(device).long()          \n",
    "\n",
    "        with autocast():\n",
    "            if train:\n",
    "                outputs = net(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    outputs = net(inputs)\n",
    "                    loss = loss_function(outputs, labels)\n",
    "        \n",
    "        predicted_labels = [torch.argmax(i).item() for i in outputs]\n",
    "        labels = labels.cpu().numpy()\n",
    "        acc = metrics.accuracy_score(labels, predicted_labels)\n",
    "        f1_score = metrics.f1_score(labels, predicted_labels, average='macro')\n",
    "        \n",
    "        losses.append(loss)\n",
    "        accuracies.append(acc)\n",
    "        f1_scores.append(f1_score)\n",
    "        \n",
    "        if train and (step+1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "        # gradient accumulation to train with bigger effective batch size\n",
    "        # with less memory use\n",
    "        # fp16 is used to speed up training and reduce memory consumption\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()            \n",
    "            logging.info(f\"Step {step} of {len(train_dataloader)},\\t\"\\\n",
    "                        f\"Accuracy: {sum(accuracies)/len(accuracies):.2f},\\t\"\\\n",
    "                        f\"Loss: {sum(losses)/len(losses):.2f},\\t\"\\\n",
    "                        f\"F1: {sum(f1_scores)/len(f1_scores):.2f}\")\n",
    "\n",
    "    return losses, accuracies, f1_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Epoch 1\n",
      "INFO:root:Training\n",
      "INFO:root:Step 9 of 625,\tAccuracy: 0.00,\tLoss: 6.22,\tF1: 0.00\n",
      "INFO:root:Step 19 of 625,\tAccuracy: 0.00,\tLoss: 6.22,\tF1: 0.00\n",
      "INFO:root:Step 29 of 625,\tAccuracy: 0.00,\tLoss: 6.22,\tF1: 0.00\n",
      "INFO:root:Step 39 of 625,\tAccuracy: 0.00,\tLoss: 6.23,\tF1: 0.00\n",
      "INFO:root:Step 49 of 625,\tAccuracy: 0.00,\tLoss: 6.23,\tF1: 0.00\n",
      "INFO:root:Step 59 of 625,\tAccuracy: 0.00,\tLoss: 6.23,\tF1: 0.00\n",
      "INFO:root:Step 69 of 625,\tAccuracy: 0.00,\tLoss: 6.23,\tF1: 0.00\n",
      "INFO:root:Step 79 of 625,\tAccuracy: 0.00,\tLoss: 6.23,\tF1: 0.00\n",
      "INFO:root:Step 89 of 625,\tAccuracy: 0.00,\tLoss: 6.23,\tF1: 0.00\n",
      "INFO:root:Step 99 of 625,\tAccuracy: 0.00,\tLoss: 6.22,\tF1: 0.00\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-191ba6fad27d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-57ed0b59460e>\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validating\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-ce99f3475d12>\u001b[0m in \u001b[0;36mforward_pass\u001b[1;34m(dataloader, train)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mf1_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-a6574db5ed9b>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;31m# fast BGR to RGB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"best.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    predicted_labels_l = []\n",
    "    labels_l = []\n",
    "    loss_l = []\n",
    "\n",
    "    for step, batch in tqdm_notebook((enumerate(dataloader))):\n",
    "        inputs = batch[0].to(device).float()\n",
    "        labels = batch[1].to(device).long()          \n",
    "\n",
    "        with autocast():            \n",
    "            with torch.no_grad():\n",
    "                outputs = net(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "        \n",
    "        predicted_labels = [torch.argmax(i).item() for i in outputs]\n",
    "        labels = labels.cpu().numpy()\n",
    "        \n",
    "        predicted_labels_l.append(predicted_labels)\n",
    "        labels_l.append(labels)\n",
    "        loss_l.append(loss)\n",
    "\n",
    "    avg_loss = sum(loss_l)/len(loss_l)\n",
    "    clf_report = metrics.classification_report(labels_l, predicted_labels_l) \n",
    "\n",
    "    return avg_loss, clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "test_losses, test_clf_report = test(dataloader=test_dataloader)\n",
    "total_time = time.time() - start\n",
    "\n",
    "logging.info(\"\\n\" + test_clf_report)\n",
    "logging.info(f\"Average inference time is: {total_time/len(test_dataloader):.3f}\")\n",
    "logging.info(f\"Test loss: {test_losses:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "test_losses, test_clf_report = test(dataloader=test_dataloader_balanced)\n",
    "total_time = time.time() - start\n",
    "\n",
    "logging.info(\"\\n\" + test_clf_report)\n",
    "logging.info(f\"Average inference time is: {total_time/len(test_dataloader_balanced):.3f}\")\n",
    "logging.info(f\"Test accuracy: {test_accuracies:.2f} | Test loss: {test_losses:.2f} | Test F1: {test_f1:.2f}\")"
   ]
  }
 ]
}